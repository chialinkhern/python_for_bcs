This lab concerns a k-nearest neighbor classification algorithm.

It is useful in situations where you know the label, category, or classification, of some items, but not others.

If you want to pick the category to which on item belongs:
    - create some sort of similarity space for all the items, in terms of their features, properties, or measurements
    - choose a number of neighbors, n, you will consider
    - look at the n nearest labeled neighbors to the item you are curious about
    - tabulate which category is most frequent among the neighbors, and choose that category as your guess

Before you start, you need to install the heatmapcluster module using
    python -m pip install heatmapcluster
or whatever version of that line you had working for installing modules.

Lab Activities and Questions

0. First, look at the data file data1.csv. What are the categories, what are the features, and what is your intuition
    about which features are going to be most useful at categorizing the items based on the data.

1. Run the program. It will generate two plots, and print out some feature correlations. Interpret the plots.
    In classify.py, you can change the values of the variables f1 and f2, and that will plot other features. You
    can also change C_INDEX and F_INDEX, and it will change what prints in the second plot.
    What do you notice? Are your intuitions the same about which features will be the best?

2. Now uncomment lines 58-61. This does a number of things:
    - divides the dataset into a training set, and a test set, using the TRAINING_PROPORTION parameter on line 17
    - This will run a number of different knn models on each training set, each one using a different number of
        nearest neighbors (determined by the min and max numbers in line 39).
    - It will then pick which one did the best and use that number of nearest neighbors on the test set.

3. How did the k-nearest neighbor algorithm perform at classifying the categories?

4. Now change to dataset data3.csv. Repeat steps 0-2, and answer those questions again using this dataset, highlight
    any big differences you notice.

5. Continue to use dataset data3.csv. Change the training proportion parameter, trying values of 0.1, 0.25, 0.5, 0.75,
    and 0.90. How does this affect the performance of the knn algorithm? What is your explanation for what is happening?

6. Reset training proportion to 0.75. Now set the normalize parameter on line 18 to true. Repeat steps 0-3. What is
    normalizing doing (you can figure out by looking for that function in the code). What is different about the
    situation, and why might normalizing help? How does it affect performance on knn?